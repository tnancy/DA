# -*- coding: utf-8 -*-
"""music.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oZTvpoeDNgN_mGpi28HhRB7ozOEJZKCv
"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

"""About this file
Contains features for the following ten genres: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae & rock.

About this file
Contains features for the following ten genres: blues, classical, country, disco, hiphop, jazz, metal, pop, reggae & rock.

tempo is the speed at which a passage of music is played

beats is the rythmic unit in music

chroma_stft Short Time Fourier Transform

rmse is Root Mean Square Error

spectral_centroid Indicates where the "center of mass" of the spectrum is located.

spectral_bandwidth It is the Wavelength interval in which a radiated spectral quantity is not less than half its maximum value

rolloff is the steepness of a transmission function with frequency

zero_crossing_rate The rate at which the signal changes from positive to negative or back

mfcc1 Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
#sets the backend of matplotlib to the inline backend
# %matplotlib inline
warnings.filterwarnings('ignore')
import statsmodels.api as sm

# Loading the data
df = pd.read_csv("music.csv", encoding= 'unicode_escape')
print(df.shape)
df.head(7)

df.info()

#prints the summary statistic of the numerical variables
df.describe(include='all')

#checking datatype for columns
df.dtypes

#variance
df.var()

#getting independent values from dataset
x = df.iloc[:, :-1].values
print(x)

#getting dependent values from dataset
y = df.iloc[:,-1].values
print(y)

#checking for missing values
df.isnull()

#count for missing values
print(df.isnull().sum())

df = df.dropna()    # Dropping the missing values.
df.count()

print(df.isnull().sum())   # After dropping the values

#detecting outliers for math score column
sns.boxplot(x=df['beats'])

#detecting outliers for reading score column
sns.boxplot(x=df['tempo'])

#Transformation - Binning
data = df['beats']
data = data[:30] #taking only first 30 data items
data=np.sort(data)
print('Data:')
print(data)
print('')

#create three different matrices having 10 rows and 3 columns
b1=np.zeros((10,3)) 
b2=np.zeros((10,3)) 
b3=np.zeros((10,3)) 

#binning by mean
for i in range (0,30,3): 
  k=int(i/3) 
  mean=(data[i] + data[i+1] + data[i+2] )/3
  for j in range(3): 
    b1[k,j]=mean 
print("----Binning by mean:---- \n",b1)
print('')

#binning by median
for i in range (0,30,3): 
  k=int(i/3) 
  for j in range (3): 
    b2[k,j]=data[i+1] 
print("----Binning by median:---- \n",b2)
print('')

#binning by boundary
for i in range (0,30,3): 
  k=int(i/3) 
  for j in range (3): 
    if (data[i+j]-data[i]) < (data[i+2]-data[i+j]): 
      b3[k,j]=data[i] 
    else: 
      b3[k,j]=data[i+2]   
print("----Binning by boundary:----\n",b3)

#Transformation - Normalization
import statistics
from sklearn import preprocessing

#min-max normalization
def minMaxNor(num,list):
    minNum=float(input("Enter Minimun Setting:\t"))
    maxNum = float(input("Enter Maximum Setting:\t"))
    ans=round(((num-min(list))/(max(list)-min(list))*(maxNum-minNum))+minNum,2)
    return ans
#df = pd.read_csv("StudentsPerformance.csv")
data = df['tempo']
data = data[:10] #taking only first 10 data items
data=np.sort(data)
print(data)

#z-score normalization
def zNor (num,mean,stdDv):
    return round((num-mean)/stdDv,2)

#modified z-score normalization
def zNorMAD (num,mean,abMeanDiv):
    return round((num-mean)/abMeanDiv,2)

#decimal-scaling normalization
def decNor(num,maxNum):
    digit=len(str(maxNum))
    div=pow(10,digit)
    return num/div

num=float(input("Enter an item from data : \t"))
if num in data:
  print("Calculating  min-max normalization")
  print("After doing min-max normalization :",minMaxNor(num,data))
  print("\nCalculating z-score normalization")
  print("After doing z-score normalization : \t", zNor(num,statistics.mean(data),statistics.stdev(data)))
  print("\nCalculating Modified z-score normalization")
  df = pd.DataFrame(data)
  print("After doing Modified z-score normalization : \t", zNorMAD(num,statistics.mean(data),df.mad()))
  print("\nCalculating decimal scaling normalization")
  print("After doing decimal scaling normalization : \t", decNor(num,max(data)))
else:
  print("Item entered is not present!!")
  print("Can't perform normalization on the selected item!")

#pie charts of various columns
plt.figure(figsize=(25,15))

#subplot(nrows, ncols, plot_number) 
plt.subplot(231)
plt.title("Genre",fontsize=30)
df = pd.read_csv("music.csv")
df['label'].value_counts().plot.pie(autopct="%.2f%%",fontsize=12)
plt.show()

#finding no. of students who got a particular score in each subject
plt.figure(figsize=(20,10))
sns.countplot(x='beats',data=df,palette="pastel")
plt.xticks(fontsize=5)
plt.show()

import seaborn as sns
plt.figure(figsize=(15,9))
sns.heatmap(df.corr(),cmap='Blues',annot=False)

#Quality correlation matrix
k = 12 #number of variables for heatmap
cols = df.corr().nlargest(k, 'mfcc6')['mfcc6'].index
cm = df[cols].corr()
plt.figure(figsize=(10,6))
sns.heatmap(cm, annot=True, cmap = 'coolwarm')

plt.figure(figsize=(9, 8))
sns.distplot(df['mfcc6'], color='g', bins=100, hist_kws={'alpha': 0.4});
plt.figure(figsize=(9, 8))
sns.distplot(df['mfcc14'], color='g', bins=100, hist_kws={'alpha': 0.4});
#With this information we can see that the stats are skewed right and some outliers lies above